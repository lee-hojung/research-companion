"""
Research Companion - Literature Review Assistant
================================================

Finds related papers from your Obsidian notes using semantic search,
then generates structured bullet-point summaries of their key findings
and policy implications.

Workflow:
  1. Build/update an embedding index of your notes (cached to JSON)
  2. Embed your query (policy + population) for retrieval
  3. Retrieve top-K similar notes via cosine similarity
  4. GPT-4o synthesizes 3-4 bullet summaries per relevant paper

Design note on query construction:
  - Retrieval query uses policy + population only (NOT your findings).
    This ensures we find papers studying similar policies/populations,
    not papers with similar results ‚Äî which would introduce circularity.
  - Your findings are passed only to the GPT synthesis stage for context.

Author: Hojung Lee & Claude
Last Updated: 2026
"""

from openai import OpenAI
import os
import re
import json
import math
import time
from datetime import datetime
from pathlib import Path

# ==========================================
# [CONFIGURATION] ‚Äî loaded from config.py
# ==========================================
from config import OPENAI_KEY, OBSIDIAN_FOLDER, INDEX_FILE

TOP_K = 8               # Candidate papers retrieved before GPT filtering
EMBEDDING_MODEL = 'text-embedding-3-small'
# ==========================================

client = OpenAI(api_key=OPENAI_KEY)


# ---- Note parsing ----

def parse_note(filepath):
    """
    Parse an Obsidian markdown note generated by step1_analysis.
    Extracts title, author, year, keywords, and content sections.
    Returns a dict ready for embedding.
    """
    with open(filepath, 'r', encoding='utf-8') as f:
        content = f.read()

    # Parse YAML frontmatter
    title, author, year = '', '', ''
    fm_match = re.match(r'^---\n(.*?)\n---', content, re.DOTALL)
    if fm_match:
        fm = fm_match.group(1)
        for line in fm.splitlines():
            if line.startswith('title:'):
                title = line.split(':', 1)[1].strip()
            elif line.startswith('author:'):
                author = line.split(':', 1)[1].strip()
            elif line.startswith('year:'):
                year = line.split(':', 1)[1].strip()

    # Fallback: derive title from filename
    if not title:
        title = Path(filepath).stem

    # Extract Obsidian [[keywords]]
    keywords = re.findall(r'\[\[([^\]]+)\]\]', content)

    # Extract named sections from the body (after frontmatter)
    body = re.sub(r'^---\n.*?\n---\n', '', content, flags=re.DOTALL)
    sections = {}
    header_re = re.compile(r'^#{1,3}\s+(.+)$', re.MULTILINE)
    headers = list(header_re.finditer(body))
    for i, h in enumerate(headers):
        sec_name = h.group(1).strip().lower()
        start = h.end()
        end = headers[i + 1].start() if i + 1 < len(headers) else len(body)
        sections[sec_name] = body[start:end].strip()

    # Build embedding text.
    # Priority order: title > keywords > research questions > findings > methodology
    # This reflects the similarity criteria: policy area > method > population.
    embed_parts = [f"Title: {title}"]
    if keywords:
        embed_parts.append(f"Keywords: {', '.join(keywords)}")

    SECTION_PRIORITY = [
        'research question',
        'key finding',
        'finding',
        'methodology',
        'method',
        'population',
        'significance',
        'summary',
    ]
    added = set()
    for priority_key in SECTION_PRIORITY:
        for sec_name, sec_text in sections.items():
            if priority_key in sec_name and sec_name not in added and sec_text:
                embed_parts.append(sec_text[:500])
                added.add(sec_name)
                break

    return {
        'filepath': str(filepath),
        'title': title,
        'author': author,
        'year': year,
        'keywords': keywords,
        'sections': sections,
        'embed_text': '\n'.join(embed_parts),
        'mtime': os.path.getmtime(filepath),
    }


# ---- Embedding & index ----

def cosine_similarity(a, b):
    dot = sum(x * y for x, y in zip(a, b))
    norm_a = math.sqrt(sum(x * x for x in a))
    norm_b = math.sqrt(sum(x * x for x in b))
    return dot / (norm_a * norm_b) if norm_a and norm_b else 0.0


def get_embedding(text):
    response = client.embeddings.create(model=EMBEDDING_MODEL, input=text)
    return response.data[0].embedding


def build_or_update_index(notes_folder, index_file):
    """
    Build or incrementally update the embedding index.

    - Skips files starting with '_' (lit review outputs from this script).
    - Only re-embeds files whose modification time changed since last run.
    - Removes index entries for deleted files.
    """
    md_files = [
        p for p in Path(notes_folder).glob('*.md')
        if not p.name.startswith('_')
    ]

    # Load existing index
    index = {}
    if os.path.exists(index_file):
        with open(index_file, 'r', encoding='utf-8') as f:
            index = json.load(f)

    updated = 0
    for fp in md_files:
        note = parse_note(fp)
        key = str(fp)
        # Re-embed only if file is new or was modified
        if index.get(key, {}).get('mtime', 0) < note['mtime']:
            print(f"  Indexing: {fp.name}")
            note['embedding'] = get_embedding(note['embed_text'])
            index[key] = note
            updated += 1
            time.sleep(0.05)  # stay within rate limits

    # Remove entries for deleted files
    existing_paths = {str(fp) for fp in md_files}
    index = {k: v for k, v in index.items() if k in existing_paths}

    with open(index_file, 'w', encoding='utf-8') as f:
        json.dump(index, f, ensure_ascii=False, indent=2)

    print(f"‚úÖ Index ready: {len(index)} notes ({updated} updated)\n")
    return index


def search(index, query_text, top_k):
    """Return top-K notes ranked by cosine similarity to query_text."""
    query_emb = get_embedding(query_text)
    scored = [
        (cosine_similarity(query_emb, note['embedding']), note)
        for note in index.values()
        if note.get('embedding')
    ]
    scored.sort(key=lambda x: x[0], reverse=True)
    return scored[:top_k]


# ---- GPT-4o synthesis ----

def format_note_for_prompt(note):
    """Compact note summary for the synthesis prompt."""
    s = note.get('sections', {})
    findings_text, method_text = '', ''
    for sec_name, sec_text in s.items():
        if 'finding' in sec_name and not findings_text:
            findings_text = sec_text[:800]
        if 'method' in sec_name and not method_text:
            method_text = sec_text[:400]

    return (
        f"Paper: {note['title']} ({note['author']}, {note['year']})\n"
        f"Keywords: {', '.join(note['keywords'][:10])}\n"
        f"Key Findings: {findings_text}\n"
        f"Methodology: {method_text}"
    ).strip()


def generate_review(matched_notes, my_policy, my_findings, my_population):
    """Generate structured literature review via GPT-4o."""
    print("üß† Generating literature review...")

    notes_text = '\n\n---\n\n'.join(
        format_note_for_prompt(note) for _, note in matched_notes
    )

    population_line = f"\nTarget population: {my_population}" if my_population else ""

    prompt = f"""You are an expert supporting an education policy researcher.

The researcher is studying:
Policy: {my_policy}{population_line}
Their findings: {my_findings}

Below are candidate papers from their literature database.
For each paper that is MEANINGFULLY related to the researcher's policy or population:

1. Write the citation as: **Author (Year)** ‚Äî *Title*
2. Provide 3-4 bullet points:
   - Main finding most relevant to the researcher's study
   - How the methodology compares or contrasts
   - Policy implication in the context of the researcher's findings

Guidelines:
- Skip papers that are only tangentially related
- Be concise ‚Äî output will be used directly in academic writing
- Ground every bullet strictly in the paper content below; do not fabricate
- Use plain academic English

CANDIDATE PAPERS:
{notes_text}
"""

    response = client.chat.completions.create(
        model='gpt-4o',
        messages=[
            {
                'role': 'system',
                'content': (
                    'You are a research assistant for an education policy scholar '
                    'specializing in causal inference and school finance. '
                    'Summarize only what is stated in the provided paper content. '
                    'Do not add information from outside the provided texts.'
                )
            },
            {'role': 'user', 'content': prompt}
        ]
    )
    return response.choices[0].message.content


# ---- Main ----

def main():
    print('=' * 60)
    print('üìö Literature Review Assistant')
    print('=' * 60)

    # Build or update the embedding index
    print('\nüîç Checking note index...')
    index = build_or_update_index(OBSIDIAN_FOLDER, INDEX_FILE)

    if not index:
        print('‚ùå No notes found. Run step1_analysis.py first to generate notes.')
        return

    # Collect research context
    print('=' * 60)
    print('Describe your research:\n')
    my_policy = input('Policy being studied:\n> ').strip()
    my_population = input('\nTarget population (optional ‚Äî press Enter to skip):\n> ').strip()
    my_findings = input('\nYour key findings:\n> ').strip()

    if not my_policy or not my_findings:
        print('‚ùå Policy and findings are required.')
        return

    # Retrieval query: policy + population only (no findings ‚Äî see module docstring)
    query_parts = [f'Education policy study: {my_policy}']
    if my_population:
        query_parts.append(f'Population: {my_population}')
    query_text = '\n'.join(query_parts)

    # Semantic search
    print(f'\nüîé Searching {len(index)} notes for similar papers...')
    results = search(index, query_text, TOP_K)

    print('\nCandidate papers (ranked by similarity):')
    for sim, note in results:
        print(f"  [{sim:.3f}] {note['author']} ({note['year']}) ‚Äî {note['title'][:55]}")

    # GPT-4o synthesis
    print()
    review_text = generate_review(results, my_policy, my_findings, my_population)

    # Save as a new Obsidian note (prefixed with '_' so the index skips it)
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    safe_policy = re.sub(r'[/\\:*?"<>|]', '-', my_policy[:40])
    outfile = os.path.join(OBSIDIAN_FOLDER, f'_litreview_{safe_policy}_{timestamp}.md')

    population_fm = f'\npopulation: {my_population}' if my_population else ''
    population_body = f'**Population:** {my_population}  \n' if my_population else ''

    header = f"""---
date: {datetime.now().strftime('%Y-%m-%d')}
policy: {my_policy}{population_fm}
tags: [lit-review, auto-generated]
---
# Literature Review: {my_policy}

**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M')}
**My findings:** {my_findings}
{population_body}
---

"""

    with open(outfile, 'w', encoding='utf-8') as f:
        f.write(header + review_text)

    print('\n' + '=' * 60)
    print(review_text)
    print('=' * 60)
    print(f'\n‚úÖ Saved to: {outfile}')


if __name__ == '__main__':
    main()
